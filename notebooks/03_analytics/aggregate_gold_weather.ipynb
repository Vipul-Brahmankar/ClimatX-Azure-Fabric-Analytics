from pyspark.sql.functions import (
    col, avg, sum, max, min, 
    date_format, current_timestamp, 
    lag, lead, round, desc
)
from pyspark.sql.window import Window

# --- CONFIGURATION ---
silver_table_name = "ClimatX_Lakehouse.Silver_Weather_Cleaned"
gold_monthly_table = "ClimatX_Lakehouse.Gold_Weather_Monthly_Summary"
gold_daily_analytics = "ClimatX_Lakehouse.Gold_Weather_Daily_Trends"

print(f"ðŸš€ Starting Gold Layer Analytics...")

# Load Silver Data
df_silver = spark.read.table(silver_table_name)

# --- ANALYTIC 1: MONTHLY AGGREGATIONS (For Executive Dashboards) ---
# Group by City and Month to get high-level trends
df_monthly = df_silver.groupBy("city", "year", "month") \
    .agg(
        round(avg("avg_temp_c"), 2).alias("avg_monthly_temp"),
        round(sum("precipitation_mm"), 2).alias("total_monthly_rain"),
        max("max_temp_c").alias("hottest_day_temp"),
        min("min_temp_c").alias("coldest_day_temp")
    ) \
    .withColumn("report_date", date_format(col("year").cast("string") + "-" + col("month").cast("string") + "-01", "yyyy-MM-dd")) \
    .orderBy("city", "year", "month")

# Write Monthly Gold Table
print(f"ðŸ“Š Saving Monthly Aggregates to: {gold_monthly_table}...")
df_monthly.write.format("delta").mode("overwrite").saveAsTable(gold_monthly_table)


# --- ANALYTIC 2: ADVANCED TIME SERIES (Rolling Averages & Trends) ---
# We use Window Functions here - a key skill for Data Engineers!

# Define a Window: Partition by City, Order by Date
window_spec = Window.partitionBy("city").orderBy("date")

# Calculate 7-Day Rolling Average (Smooths out noise in the data)
# "rowsBetween(-6, 0)" means: Look at today plus the previous 6 days
df_daily_trends = df_silver.withColumn(
    "rolling_7day_avg_temp", 
    round(avg("avg_temp_c").over(window_spec.rowsBetween(-6, 0)), 2)
)

# Calculate "Temperature Change vs Yesterday" (Lag Function)
df_daily_trends = df_daily_trends.withColumn(
    "prev_day_temp", 
    lag("avg_temp_c", 1).over(window_spec)
).withColumn(
    "temp_change_vs_yesterday", 
    round(col("avg_temp_c") - col("prev_day_temp"), 2)
)

# Filter columns for the final Gold table (Keep it clean for Power BI)
final_gold_daily = df_daily_trends.select(
    "city", "date", "year", "month", 
    "avg_temp_c", "rolling_7day_avg_temp", 
    "temp_change_vs_yesterday", "precipitation_mm",
    "is_freezing"
)

# Write Daily Trends Gold Table
print(f"Saving Daily Trends to: {gold_daily_analytics}...")
final_gold_daily.write.format("delta").mode("overwrite").partitionBy("city").saveAsTable(gold_daily_analytics)

print(f"Gold Layer Complete!")
print("   1. Monthly Summary (Best for Bar Charts/KPIs)")
print("   2. Daily Trends (Best for Line Charts with Smoothing)")

# --- VERIFICATION ---
print("Preview of Rolling Averages:")
display(spark.read.table(gold_daily_analytics).filter("city = 'London'").orderBy(desc("date")).limit(5))
