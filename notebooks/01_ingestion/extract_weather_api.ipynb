import requests
import pandas as pd
from pyspark.sql.functions import col, to_date, current_timestamp

# --- CONFIGURATION ---
# We will fetch data for: London, New York, Tokyo
locations = [
    {"city": "London", "lat": 51.5074, "lon": -0.1278},
    {"city": "New York", "lat": 40.7128, "lon": -74.0060},
    {"city": "Tokyo", "lat": 35.6762, "lon": 139.6503}
]

# API Endpoint: Open-Meteo Historical Weather
URL = "https://archive-api.open-meteo.com/v1/archive"

# Date Range: Last 5 Years (Adjust as needed)
start_date = "2020-01-01"
end_date = "2024-12-31"

all_data = []

print("üöÄ Starting Data Extraction...")

# --- STEP 1: EXTRACT (Fetch from API) ---
for loc in locations:
    print(f"Fetching data for {loc['city']}...")
    
    params = {
        "latitude": loc["lat"],
        "longitude": loc["lon"],
        "start_date": start_date,
        "end_date": end_date,
        "daily": ["temperature_2m_max", "temperature_2m_min", "precipitation_sum"],
        "timezone": "auto"
    }
    
    response = requests.get(URL, params=params)
    
    if response.status_code == 200:
        data = response.json()
        daily = data['daily']
        
        # Create a temporary list of dictionaries for this city
        for i in range(len(daily['time'])):
            all_data.append({
                "city": loc["city"],
                "date": daily['time'][i],
                "max_temp_c": daily['temperature_2m_max'][i],
                "min_temp_c": daily['temperature_2m_min'][i],
                "precipitation_mm": daily['precipitation_sum'][i],
                "latitude": loc["lat"],
                "longitude": loc["lon"]
            })
    else:
        print(f"‚ùå Failed to fetch {loc['city']}: {response.status_code}")

print(f"Extraction Complete. Processed {len(all_data)} records.")

# --- STEP 2: DATAFRAME CONVERSION ---
# Convert list to Pandas DataFrame first
pdf = pd.DataFrame(all_data)

# Convert Pandas DF to Spark DataFrame (Distributed format)
spark_df = spark.createDataFrame(pdf)

# --- STEP 3: LOAD (Save to Lakehouse) ---
# Add ingestion metadata
final_df = spark_df.withColumn("ingestion_timestamp", current_timestamp()) \
                   .withColumn("date", to_date(col("date")))

# Define Table Name (Bronze Layer)
table_name = "ClimatX_Lakehouse.Bronze_Historical_Weather"

# Write to Delta Table (Upsert/Overwrite logic can be added later; here we overwrite)
# Note: Ensure you have a Lakehouse attached to this notebook!
print(f"Saving to Delta Table: {table_name}...")
final_df.write.mode("overwrite").format("delta").saveAsTable(table_name)

print("Success! Data is now in your Lakehouse.")

# Display the first 5 rows to verify
display(final_df.limit(5))
