from pyspark.sql.functions import (
    col, to_date, year, month, dayofmonth, 
    current_timestamp, round, when, avg
)
from pyspark.sql.window import Window

# --- CONFIGURATION ---
bronze_table_name = "ClimatX_Lakehouse.Bronze_Historical_Weather"
silver_table_name = "ClimatX_Lakehouse.Silver_Weather_Cleaned"

print(f"Starting Silver Layer Transformation...")

# --- STEP 1: READ BRONZE DATA ---
# Read from the Delta table created in the ingestion step
try:
    df_bronze = spark.read.table(bronze_table_name)
    print(f"Successfully loaded {bronze_table_name}")
except Exception as e:
    print(f"Error loading Bronze table: {e}")
    # Stop execution if bronze table is missing
    dbutils.notebook.exit("Bronze table not found") 

# --- STEP 2: DATA CLEANING & QUALITY CHECKS ---

# A. Deduplication
# Ensure we don't have duplicate entries for the same city on the same date
df_deduped = df_bronze.dropDuplicates(['city', 'date'])

# B. Handle Missing Values (Imputation)
# If 'precipitation_mm' is null, assume 0.0 (common weather data practice)
df_cleaned = df_deduped.fillna(0.0, subset=['precipitation_mm'])

# C. Data Quality Rules (Filter invalid data)
# Example: Filter out unrealistic temperatures (e.g., > 60°C or < -90°C)
df_filtered = df_cleaned.filter(
    (col("max_temp_c") < 60) & (col("min_temp_c") > -90)
)

# --- STEP 3: FEATURE ENGINEERING (ENRICHMENT) ---

df_enriched = df_filtered \
    .withColumn("date", to_date(col("date"))) \
    .withColumn("year", year(col("date"))) \
    .withColumn("month", month(col("date"))) \
    .withColumn("day", dayofmonth(col("date"))) \
    .withColumn("avg_temp_c", round((col("max_temp_c") + col("min_temp_c")) / 2, 2)) \
    .withColumn("is_freezing", when(col("min_temp_c") < 0, True).otherwise(False)) \
    .withColumn("processing_timestamp", current_timestamp())

# --- STEP 4: WRITE TO SILVER TABLE ---

print(f"Saving to Silver Delta Table: {silver_table_name}...")

# Write to Delta format
# 'overwrite' mode replaces the table content. In production, you might use 'merge' (upsert).
# We partition by 'city' to optimize query performance for city-specific dashboards.
df_enriched.write \
    .format("delta") \
    .mode("overwrite") \
    .partitionBy("city") \
    .option("overwriteSchema", "true") \
    .saveAsTable(silver_table_name)

print(f"Success! Silver transformation complete.")
print(f"   Table: {silver_table_name}")
print(f"   Partitions: city")

# --- VERIFICATION ---
print("Preview of Cleaned Data:")
display(spark.read.table(silver_table_name).limit(5))
